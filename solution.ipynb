{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/06/7a/442f7da21792566012e5c7e5a7dffa44c1b6cc05c0c27856bbc8a7718b28/xgboost-0.72.1-py2.py3-none-manylinux1_x86_64.whl (18.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 18.4MB 74kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scipy (from xgboost)\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/32/5196b64476bd41d596a8aba43506e2403e019c90e1a3dfc21d51b83db5a6/scipy-1.1.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting numpy (from xgboost)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/b9/479ccb55cc7dcff3d4fc7c8c26d4887846875e7d4f04483a36f335bed712/numpy-1.15.0-cp35-cp35m-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.8MB 104kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, xgboost\n",
      "Successfully installed numpy-1.15.0 scipy-1.1.0 xgboost-0.72.1\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install modules\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "import matplotlib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  59055\n",
      "predicting data size:  25310\n"
     ]
    }
   ],
   "source": [
    "######################_getting_data_from_files_######################\n",
    "# training data\n",
    "train_file_path = 'train.csv'\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "print(\"training data size: \",len(train_data))\n",
    "      \n",
    "# test data\n",
    "predicting_data_file_path='test.csv'\n",
    "predicting_data = pd.read_csv(predicting_data_file_path)\n",
    "print(\"predicting data size: \",len(predicting_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_selecting_affecting_columns_##################\n",
    "\n",
    "# any variable updated (or created) after the target value is realized should be excluded to prevent Leaky Predictors\n",
    "\n",
    "# to get the size of data\n",
    "# train_data.shape\n",
    "all_cols=train_data.columns \n",
    "\n",
    "# if selecting all columns drop predicting column\n",
    "predicting_cols=all_cols.drop(['trip_duration']) \n",
    "\n",
    "# predicting_cols=['LotArea', 'YearBuilt','1stFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_X_y_predictingX_##############################\n",
    "X=train_data[predicting_cols]\n",
    "y=train_data.trip_duration\n",
    "\n",
    "predicting_X_=predicting_data[predicting_cols]\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_convert_column_data_like_time################\n",
    "\n",
    "\n",
    "\n",
    "# date into hours\n",
    "def get_hour(x):\n",
    "    h=datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%d %H:%M:%S').time()\n",
    "    return h.hour\n",
    "\n",
    "# date into day\n",
    "def get_day(x):\n",
    "    d=datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%d %H:%M:%S').date().weekday()\n",
    "    return d\n",
    "\n",
    "X['pickup_time_hour'] = X['pickup_time'].apply(lambda x: get_hour(x))\n",
    "\n",
    "X['pickup_time_day'] = X['pickup_time'].apply(lambda x: get_day(x))\n",
    "\n",
    "del X['pickup_time']\n",
    "\n",
    "\n",
    "predicting_X_['pickup_time_hour'] = predicting_X_['pickup_time'].apply(lambda x: get_hour(x))\n",
    "\n",
    "predicting_X_['pickup_time_day'] = predicting_X_['pickup_time'].apply(lambda x: get_day(x))\n",
    "\n",
    "del predicting_X_['pickup_time']\n",
    "# after adding new column pickup_time_hour delete pickup_time column\n",
    "\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pickup_lat  pickup_long  drop_lat  drop_long  taxi_model  travel_distance  \\\n",
      "0    6.871380    79.864400  6.917770  79.848400           1             8.46   \n",
      "1    6.852475    79.952164  6.854389  79.905299           3             7.77   \n",
      "2    6.897326    79.873669  6.898695  79.916012           3             6.69   \n",
      "3    7.131190    79.876200  7.266240  79.855700           2            16.76   \n",
      "4    6.800170    79.924000  6.773550  79.924100           3             3.99   \n",
      "\n",
      "   pickup_time_hour  pickup_time_day  \n",
      "0                23                1  \n",
      "1                 7                0  \n",
      "2                14                5  \n",
      "3                 2                5  \n",
      "4                18                1  \n",
      "   pickup_lat  pickup_long  drop_lat  drop_long  taxi_model  travel_distance  \\\n",
      "0    6.885844    79.926211  6.887515  79.907675           3             8.25   \n",
      "1    6.919840    79.870200  6.944230  79.861500           1             4.57   \n",
      "2    6.974720    79.877127  6.986847  79.889974           3             3.50   \n",
      "3    6.899970    79.876000  6.801380  79.921600           4            16.19   \n",
      "4    6.989940    79.893400  7.004720  79.880800           3             3.57   \n",
      "\n",
      "   pickup_time_hour  pickup_time_day  \n",
      "0                13                4  \n",
      "1                13                5  \n",
      "2                20                5  \n",
      "3                13                6  \n",
      "4                20                1  \n"
     ]
    }
   ],
   "source": [
    "######################_handling_categorial_##########################\n",
    "# dtypes of colmns\n",
    "# print(X.dtypes)\n",
    "# print(len(X))\n",
    "\n",
    "# # handling categorial cols using one_hot_encoding\n",
    "# one_hot_encoded_X = pd.get_dummies(X)\n",
    "# one_hot_encoded_predicting_X = pd.get_dummies(predicting_X_)\n",
    "# X_encoded, predicting_X_encoded = one_hot_encoded_X.align(one_hot_encoded_predicting_X, join='left', axis=1)\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# # handling categorial cols using label LabelEncoder\n",
    "X_encoded=X\n",
    "predicting_X_encoded=predicting_X_\n",
    "\n",
    "label_encoder_taxi = LabelEncoder()\n",
    "label_encoder_time = LabelEncoder()\n",
    "X_encoded['taxi_model']=label_encoder_taxi.fit_transform(X.taxi_model.values)\n",
    "# X_encoded['pickup_time']=label_encoder_time.fit_transform(X.pickup_time.values)\n",
    "# print(X_encoded.head())\n",
    "\n",
    "predicting_X_encoded['taxi_model']=label_encoder_taxi.transform(predicting_X_.taxi_model.values)\n",
    "# predicting_X_encoded['pickup_time']=label_encoder_time.transform(predicting_X_.pickup_time.values)\n",
    "# print(predicting_X_encoded.head())\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################_deviding_data_X_y_############################\n",
    "# deviding training data for checking correctness\n",
    "train_X,test_X,train_y,test_y=train_test_split(X_encoded,y,random_state=0)\n",
    "\n",
    "#required data for prediction\n",
    "predicting_X=predicting_X_encoded\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_null_col_handling_############################\n",
    "# #no use if using pipeline\n",
    "# handling null columns\n",
    "# checking null cols #train_data.isnull().any()\n",
    "\n",
    "# droping\n",
    "# cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
    "# reduced_trin_data = train_data.drop(cols_with_missing, axis=1)\n",
    "# reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# using SimpleImputer\n",
    "\n",
    "# for training data\n",
    "imputed_X_train = train_X.copy()\n",
    "imputed_X_test = test_X.copy()\n",
    "\n",
    "# for predicting data\n",
    "imputed_predicting_X=predicting_X.copy()\n",
    "\n",
    "# cols with null in training data\n",
    "cols_with_missing_training_data = (col for col in train_X.columns if train_X[col].isnull().any())\n",
    "\n",
    "# cols with null in predicting data\n",
    "cols_with_missing_predicting_data=(col for col in predicting_X.columns if predicting_X[col].isnull().any())\n",
    "\n",
    "# set of all null cols\n",
    "cols_with_missing=list(set(list(cols_with_missing_training_data)+list(cols_with_missing_predicting_data)))\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train[col + '_was_missing'] = imputed_X_train[col].isnull()\n",
    "    imputed_X_test[col + '_was_missing'] = imputed_X_test[col].isnull()\n",
    "    imputed_predicting_X[col + '_was_missing'] = imputed_predicting_X[col].isnull()\n",
    "\n",
    "imputer = Imputer()\n",
    "# imputering data in training data\n",
    "imputed_X_train = imputer.fit_transform(imputed_X_train) #fit imputer and transform data\n",
    "imputed_X_test = imputer.transform(imputed_X_test) #transform data\n",
    "\n",
    "# imputering data in predicting data\n",
    "imputed_predicting_X = imputer.transform(imputed_predicting_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_model_selection_##############################\n",
    "def get_mae_model(model_,X, y):\n",
    "\t# model=DecisionTreeRegressor()\n",
    "    model=model_\n",
    "    mae_val= -1 * cross_val_score(model, X, y, scoring = 'neg_mean_absolute_error').mean()\n",
    "    print(\"MAE with \",type(model).__name__,\" \\t:\",mae_val)\n",
    "    return mae_val\n",
    "\n",
    "# finding max leaf nodes\n",
    "def get_MAE_nodes(max_leaf_nodes, training_X, predicting_X, training_y, predicting_values_y):\n",
    "    model=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n",
    "    model.fit(training_X,training_y)\n",
    "    predicted_val_y=model.predict(predicting_X)\n",
    "    MAE=mean_absolute_error(predicting_values_y,predicted_val_y)\n",
    "    return (MAE)\n",
    "# for max_leaf_nodes in [5,50,60,65,67,70,75,80,85,90]:\n",
    "#     current_MAE=get_MAE_nodes(max_leaf_nodes,train_X,val_X,train_y,val_y)\n",
    "#     print(\"max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes,current_MAE))\n",
    "\n",
    "# get_mae_model(DecisionTreeRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(RandomForestRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(XGBRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(GradientBoostingRegressor(),imputed_X_train,train_y)\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=2000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################_trainging_the_model_##########################\n",
    "# find the number of nodes for least MAE and create the model according to it\n",
    "# model=DecisionTreeRegressor(max_leaf_nodes=75,random_state=0)  #model with specifying max leaf nodes\n",
    "# model = GradientBoostingRegressor()\n",
    "# model.fit(imputed_X_train, train_y)\n",
    "# scpecify n_jobs for XGBRegressor if dataset is too large. assign num of cores in machine to n_jobs \n",
    "# find using early_stopping_rounds and assign it to n_estimators, start with a big number\n",
    "model=XGBRegressor(n_estimators=2000,learning_rate=0.05)\n",
    "# model=XGBRegressor()\n",
    "model.fit(imputed_X_train,train_y,early_stopping_rounds=20, eval_set=[(imputed_X_test, test_y)], verbose=False)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_using_pipeline_###############################\n",
    "# pipeline = make_pipeline(Imputer(), RandomForestRegressor())\n",
    "# pipeline.fit(train_X, train_y)\n",
    "# predictions = pipeline.predict(test_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################_plot_partial_dependence_######################\n",
    "# should be an instance of BaseGradientBoosting \n",
    "# cols_to_use=['LotArea', 'YearBuilt','1stFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n",
    "# cols_to_use=all_cols\n",
    "# feature is defining which should be plotted from cols_to_use\n",
    "# y_plots = plot_partial_dependence(model, features=[0,1,2,3,4,5,6,7,8], X=imputed_X_train, feature_names=cols_to_use,grid_resolution=10)\n",
    "# matplotlib.pyplot.show(block=True)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_cross_validation_#############################\n",
    "# # use cross_val_score instead of train_test_split for small size data sets\n",
    "# # this is not needed for large data sets. train_test_split is faster\n",
    "# imputer_=Imputer()\n",
    "# pipeline = make_pipeline(imputer_, XGBRegressor())\n",
    "# pipeline.fit(X_encoded, y)\n",
    "\n",
    "# # imputing predicting test using same imputer\n",
    "# imputed_predicting_X = imputer_.transform(predicting_X)\n",
    "\n",
    "# # possible methods for scoring =>'accuracy' , 'neg_mean_absolute_error'\n",
    "# scores = cross_val_score(pipeline, X_encoded, y, scoring='neg_mean_absolute_error')\n",
    "# print(scores)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE:\t 8.317606439260375\n"
     ]
    }
   ],
   "source": [
    "######################_evaluating_the_model_#########################\n",
    "predicted_values_for_traininig_set=model.predict(imputed_X_test)\n",
    "# predicted_values_for_traininig_set=pipeline.predict(imputed_X_test)\n",
    "\n",
    "# print correct and predicted values\n",
    "# print(\"actual\",'predicted\\n')\n",
    "# for idx,val in enumerate(test_y):\n",
    "# \tprint (val,predicted_values_for_traininig_set[idx])\n",
    "\n",
    "# print mean absolute error\n",
    "print(\"\\nMAE:\\t\",mean_absolute_error(test_y,predicted_values_for_traininig_set))\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_prediction_for_required_data_#################\n",
    "predicted_result=model.predict(imputed_predicting_X)\n",
    "# predicted_result=pipeline.predict(imputed_predicting_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_visualization_of_prediction_##################\n",
    "# creating a csv\n",
    "# submission = pd.DataFrame({'Id': predicting_data.Id, 'SalePrice': predicted_result})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# creating txt file\n",
    "np.savetxt('output.txt',predicted_result,fmt='%.2f')\n",
    "#####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
