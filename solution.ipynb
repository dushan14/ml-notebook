{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install modules\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dushan/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/dushan/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/dushan/venv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "import matplotlib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  59055\n",
      "predicting data size:  25310\n"
     ]
    }
   ],
   "source": [
    "######################_getting_data_from_files_######################\n",
    "# training data\n",
    "train_file_path = 'train.csv'\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "print(\"training data size: \",len(train_data))\n",
    "      \n",
    "# test data\n",
    "predicting_data_file_path='test.csv'\n",
    "predicting_data = pd.read_csv(predicting_data_file_path)\n",
    "print(\"predicting data size: \",len(predicting_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_lat         float64\n",
       "pickup_long        float64\n",
       "drop_lat           float64\n",
       "drop_long          float64\n",
       "pickup_time         object\n",
       "taxi_model          object\n",
       "travel_distance    float64\n",
       "trip_duration      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_selecting_affecting_columns_##################\n",
    "\n",
    "# any variable updated (or created) after the target value is realized should be excluded to prevent Leaky Predictors\n",
    "\n",
    "# to get the size of data\n",
    "# train_data.shape\n",
    "all_cols=train_data.columns \n",
    "\n",
    "# if selecting all columns drop predicting column\n",
    "predicting_cols=all_cols.drop(['trip_duration']) \n",
    "\n",
    "# predicting_cols=['LotArea', 'YearBuilt','1stFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_X_y_predictingX_##############################\n",
    "X=train_data[predicting_cols]\n",
    "y=train_data.trip_duration\n",
    "\n",
    "predicting_X_=predicting_data[predicting_cols]\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_convert_column_data_like_time################\n",
    "\n",
    "\n",
    "\n",
    "# date into hours\n",
    "def get_hour(x):\n",
    "    h=datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%d %H:%M:%S').time()\n",
    "    return h.hour\n",
    "\n",
    "# date into day\n",
    "def get_day(x):\n",
    "    d=datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%d %H:%M:%S').date().weekday()\n",
    "    return d\n",
    "\n",
    "X['pickup_time_hour'] = X['pickup_time'].apply(lambda x: get_hour(x))\n",
    "\n",
    "X['pickup_time_day'] = X['pickup_time'].apply(lambda x: get_day(x))\n",
    "\n",
    "del X['pickup_time']\n",
    "\n",
    "\n",
    "predicting_X_['pickup_time_hour'] = predicting_X_['pickup_time'].apply(lambda x: get_hour(x))\n",
    "\n",
    "predicting_X_['pickup_time_day'] = predicting_X_['pickup_time'].apply(lambda x: get_day(x))\n",
    "\n",
    "del predicting_X_['pickup_time']\n",
    "# after adding new column pickup_time_hour delete pickup_time column\n",
    "\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################_handling_categorial_##########################\n",
    "# dtypes of colmns\n",
    "# print(X.dtypes)\n",
    "# print(len(X))\n",
    "\n",
    "# # handling categorial cols using one_hot_encoding\n",
    "# one_hot_encoded_X = pd.get_dummies(X)\n",
    "# one_hot_encoded_predicting_X = pd.get_dummies(predicting_X_)\n",
    "# X_encoded, predicting_X_encoded = one_hot_encoded_X.align(one_hot_encoded_predicting_X, join='left', axis=1)\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# # handling categorial cols using label LabelEncoder\n",
    "X_encoded=X\n",
    "predicting_X_encoded=predicting_X_\n",
    "\n",
    "label_encoder_taxi = LabelEncoder()\n",
    "label_encoder_time = LabelEncoder()\n",
    "X_encoded['taxi_model']=label_encoder_taxi.fit_transform(X.taxi_model.values)\n",
    "# X_encoded['pickup_time']=label_encoder_time.fit_transform(X.pickup_time.values)\n",
    "# print(X_encoded.head())\n",
    "\n",
    "predicting_X_encoded['taxi_model']=label_encoder_taxi.transform(predicting_X_.taxi_model.values)\n",
    "# predicting_X_encoded['pickup_time']=label_encoder_time.transform(predicting_X_.pickup_time.values)\n",
    "# print(predicting_X_encoded.head())\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pickup_time_day</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_time_hour</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_distance</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxi_model</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drop_long</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drop_lat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_long</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_lat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total  Percent\n",
       "pickup_time_day       0      0.0\n",
       "pickup_time_hour      0      0.0\n",
       "travel_distance       0      0.0\n",
       "taxi_model            0      0.0\n",
       "drop_long             0      0.0\n",
       "drop_lat              0      0.0\n",
       "pickup_long           0      0.0\n",
       "pickup_lat            0      0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table for missing data analysis\n",
    "def draw_missing_data_table(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data\n",
    "\n",
    "draw_missing_data_table(predicting_X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_null_col_handling_############################\n",
    "# #no use if using pipeline\n",
    "# handling null columns\n",
    "# checking null cols #train_data.isnull().any()\n",
    "\n",
    "\n",
    "# droping\n",
    "# cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
    "# reduced_trin_data = train_data.drop(cols_with_missing, axis=1)\n",
    "# reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# using SimpleImputer\n",
    "\n",
    "# for training data\n",
    "imputed_X = X_encoded.copy()\n",
    "\n",
    "# for predicting data\n",
    "imputed_predicting_X=predicting_X_encoded.copy()\n",
    "\n",
    "# cols with null in training data\n",
    "cols_with_missing_training_data = (col for col in X_encoded.columns if X_encoded[col].isnull().any())\n",
    "\n",
    "# cols with null in predicting data\n",
    "cols_with_missing_predicting_data=(col for col in predicting_X_encoded.columns if predicting_X_encoded[col].isnull().any())\n",
    "\n",
    "# set of all null cols\n",
    "cols_with_missing=list(set(list(cols_with_missing_training_data)+list(cols_with_missing_predicting_data)))\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train[col + '_was_missing'] = imputed_X_train[col].isnull()\n",
    "    imputed_X_test[col + '_was_missing'] = imputed_X_test[col].isnull()\n",
    "    imputed_predicting_X[col + '_was_missing'] = imputed_predicting_X[col].isnull()\n",
    "\n",
    "imputer = Imputer()\n",
    "# imputering data in training data\n",
    "imputed_X_train = imputer.fit_transform(imputed_X) #fit imputer and transform data\n",
    "\n",
    "# imputering data in predicting data\n",
    "imputed_predicting_X = imputer.transform(imputed_predicting_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################_deviding_data_X_y_############################\n",
    "# deviding training data for checking correctness\n",
    "train_X,test_X,train_y,test_y=train_test_split(imputed_X_train,y,random_state=0)\n",
    "\n",
    "#required data for prediction\n",
    "predicting_X=imputed_predicting_X\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_model_selection_##############################\n",
    "def get_mae_model(model_,X, y):\n",
    "\t# model=DecisionTreeRegressor()\n",
    "    model=model_\n",
    "    mae_val= -1 * cross_val_score(model, X, y, scoring = 'neg_mean_absolute_error').mean()\n",
    "    print(\"MAE with \",type(model).__name__,\" \\t:\",mae_val)\n",
    "    return mae_val\n",
    "\n",
    "# finding max leaf nodes\n",
    "def get_MAE_nodes(max_leaf_nodes, training_X, predicting_X, training_y, predicting_values_y):\n",
    "    model=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n",
    "    model.fit(training_X,training_y)\n",
    "    predicted_val_y=model.predict(predicting_X)\n",
    "    MAE=mean_absolute_error(predicting_values_y,predicted_val_y)\n",
    "    return (MAE)\n",
    "# for max_leaf_nodes in [5,50,60,65,67,70,75,80,85,90]:\n",
    "#     current_MAE=get_MAE_nodes(max_leaf_nodes,train_X,val_X,train_y,val_y)\n",
    "#     print(\"max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes,current_MAE))\n",
    "\n",
    "# get_mae_model(DecisionTreeRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(RandomForestRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(XGBRegressor(),imputed_X_train,train_y)\n",
    "# get_mae_model(GradientBoostingRegressor(),imputed_X_train,train_y)\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=2000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################_trainging_the_model_##########################\n",
    "# find the number of nodes for least MAE and create the model according to it\n",
    "# model=DecisionTreeRegressor(max_leaf_nodes=75,random_state=0)  #model with specifying max leaf nodes\n",
    "# model.fit(train_X, train_y)\n",
    "\n",
    "# model= RandomForestRegressor()\n",
    "# model.fit(train_X, train_y)\n",
    "\n",
    "# model = GradientBoostingRegressor()\n",
    "# model.fit(train_X, train_y)\n",
    "\n",
    "# model=XGBRegressor()\n",
    "# scpecify n_jobs for XGBRegressor if dataset is too large. assign num of cores in machine to n_jobs \n",
    "# find using early_stopping_rounds and assign it to n_estimators, start with a big number\n",
    "model=XGBRegressor(n_estimators=2000,learning_rate=0.05)\n",
    "model.fit(train_X,train_y,early_stopping_rounds=20, eval_set=[(test_X, test_y)], verbose=False)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_using_pipeline_###############################\n",
    "# pipeline = make_pipeline(Imputer(), RandomForestRegressor())\n",
    "# pipeline.fit(train_X, train_y)\n",
    "# predictions = pipeline.predict(test_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################_plot_partial_dependence_######################\n",
    "# # should be an instance of BaseGradientBoosting \n",
    "# # cols_to_use=['travel_distance', 'pickup_time_hour','trip_duration']\n",
    "# cols_to_use=all_cols\n",
    "# # feature is defining which should be plotted from cols_to_use\n",
    "# y_plots = plot_partial_dependence(model, features=[1,2], X=train_X, feature_names=cols_to_use,grid_resolution=10)\n",
    "# matplotlib.pyplot.show(block=True)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_cross_validation_#############################\n",
    "# # use cross_val_score instead of train_test_split for small size data sets\n",
    "# # this is not needed for large data sets. train_test_split is faster\n",
    "# imputer_=Imputer()\n",
    "# pipeline = make_pipeline(imputer_, XGBRegressor())\n",
    "# pipeline.fit(X_encoded, y)\n",
    "\n",
    "# # imputing predicting test using same imputer\n",
    "# imputed_predicting_X = imputer_.transform(predicting_X)\n",
    "\n",
    "# # possible methods for scoring =>'accuracy' , 'neg_mean_absolute_error'\n",
    "# scores = cross_val_score(pipeline, X_encoded, y, scoring='neg_mean_absolute_error')\n",
    "# print(scores)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE:\t 8.348123096250577\n"
     ]
    }
   ],
   "source": [
    "######################_evaluating_the_model_#########################\n",
    "predicted_values_for_traininig_set=model.predict(test_X)\n",
    "# predicted_values_for_traininig_set=pipeline.predict(imputed_X_test)\n",
    "\n",
    "# print correct and predicted values\n",
    "# print(\"actual\",'predicted\\n')\n",
    "# for idx,val in enumerate(test_y):\n",
    "# \tprint (val,predicted_values_for_traininig_set[idx])\n",
    "\n",
    "# print mean absolute error\n",
    "print(\"\\nMAE:\\t\",mean_absolute_error(test_y,predicted_values_for_traininig_set))\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_prediction_for_required_data_#################\n",
    "predicted_result=model.predict(predicting_X)\n",
    "# predicted_result=pipeline.predict(imputed_predicting_X)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################_visualization_of_prediction_##################\n",
    "# creating a csv\n",
    "# submission = pd.DataFrame({'Id': predicting_data.Id, 'SalePrice': predicted_result})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# creating txt file\n",
    "np.savetxt('output.txt',predicted_result,fmt='%.2f')\n",
    "\n",
    "#####################################################################\n",
    "#                                                                   #\n",
    "#                                END                                #\n",
    "#                                                                   #\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############                    ############\n",
    "############   plots examples   ############\n",
    "############                    ############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Histograms allow you to plot the distributions of numeric variables.\n",
    "\n",
    "# # HistogramPython\n",
    "# # Distribution Plot (a.k.a. Histogram)\n",
    "# sns.distplot(train_data.travel_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distribution plots  according to values on one field\n",
    "# g = sns.FacetGrid(train_data, col='taxi_model')\n",
    "# g = (g.map(sns.distplot,\"travel_distance\", hist=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reg line plot with scatter\n",
    "\n",
    "# sns.lmplot(x='travel_distance', y='trip_duration', data=train_data,scatter=False,hue='taxi_model')\n",
    "# sns.lmplot(x='travel_distance', y='trip_duration', data=train_data, fit_reg=False,hue='taxi_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "# https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166\n",
    "\n",
    "# sns.pairplot(train_data)\n",
    "#here diagonal is showing the distribution of that variable\n",
    "\n",
    "# sns.pairplot(train_data, hue = 'taxi_model')\n",
    "\n",
    "# Create a pair plot colored by taxi_model with a density plot of the # diagonal and format the scatter plots.\n",
    "# sns.pairplot(train_data, hue = 'taxi_model', \n",
    "#              vars = ['travel_distance', 'trip_duration'],\n",
    "#              diag_kind = 'kde',\n",
    "#              plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n",
    "#              size = 4)\n",
    "\n",
    "# can add constraints\n",
    "# sns.pairplot(train_data[train_data['year'] >= 2000],\n",
    "#              vars = ['travel_distance', 'trip_duration'], \n",
    "#              hue = 'continent', diag_kind = 'kde', \n",
    "#              plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n",
    "#              size = 4);\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance of the PairGrid class.\n",
    "\n",
    "# grid = sns.PairGrid(data= train_data,\n",
    "#                     vars = ['travel_distance', 'trip_duration']\n",
    "#                     , size = 4)\n",
    "# # Map a scatter plot to the upper triangle\n",
    "# grid = grid.map_upper(plt.scatter)\n",
    "\n",
    "# # Map a histogram to the diagonal\n",
    "# grid = grid.map_diag(plt.hist, bins = 10, color = 'darkred', \n",
    "#                      edgecolor = 'k')\n",
    "\n",
    "# # Map a density plot to the lower triangle\n",
    "# grid = grid.map_lower(sns.kdeplot, cmap = 'Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot heatmap\n",
    "\n",
    "# f,ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# # Calculate correlations\n",
    "# corr = train_data.corr()\n",
    " \n",
    "# # Heatmap\n",
    "# sns.heatmap(corr,annot=True,linewidths=3, fmt= '.2f',ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot heatmap only down side\n",
    "\n",
    "# f,ax = plt.subplots(figsize=(6, 6))\n",
    "# mask = np.zeros_like(corr)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# with sns.axes_style(\"white\"):\n",
    "#     ax_ = sns.heatmap(corr, mask=mask,linewidths=3,annot=True,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot result\n",
    "\n",
    "# x=[1,3,5,6,7]\n",
    "# y=[2,10,20,3,5]\n",
    "# plt.plot(x,y,'bo',x,y,'r--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
